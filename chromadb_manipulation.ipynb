{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roQgTPMVuuhl"
      },
      "outputs": [],
      "source": [
        "# pip install langchain, langchain_community,sentence_transformers\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# selected a sample inputs and just considered every element in input[list] as a document\n",
        "\n",
        "inputs = [\"Rag is genrally called as retrieval agumentation and generation\",\n",
        "        \"Rag has two main important components called as retriver and generator\",\n",
        "        \"Rag is used to build a chat bot based on your own documents\",\n",
        "        \"Rag uses LLM which takes query and context as input and generates answer.\"\n",
        "]"
      ],
      "metadata": {
        "id": "eOhFTj-UvCfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialising the embeddings using HuggingFaceEmbeddings from sentence transformer and model as all-MiniLM-L6-v2\n",
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n"
      ],
      "metadata": {
        "id": "6jwME3_UvImS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intiating chroma vector db\n",
        "vector_db = Chroma(\n",
        " collection_name=\"example_collection\", # name of our collection where embeddings of the inputs gets stored\n",
        " embedding_function=embeddings,\n",
        " persist_directory=\"./chroma_langchain_db\", # path to store the embeddings local and it is not mandatory to provide\n",
        ")\n"
      ],
      "metadata": {
        "id": "TRHISgpWvR2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Documnet Object for each of the items in your inputs\n",
        "documents = []\n",
        "for i in range(len(inputs)):\n",
        " documents.append(Document(page_content = inputs[i]))\n"
      ],
      "metadata": {
        "id": "vZV0C9IFvWyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Generation of a unique ids for every document which is very convinient while updation or deletion any of the document\n",
        " ids = [str(i) for i in range(len(documents))]"
      ],
      "metadata": {
        "id": "X7DBYr_xvdDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Add the data in a vector DB\n",
        " vector_db.add_documents(documents=documents, ids=ids)\n",
        " print(\"Data after Adding to vector store\", vector_db.get(include=['documents']))\n"
      ],
      "metadata": {
        "id": "1cWvbJrnvhvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # update Document(inputs)\n",
        "updating_new_document = Document(\n",
        "     page_content= \"Vector db is used to store embeddings\",\n",
        "     metadata={'chunk': 'update'}\n",
        " )\n",
        "\n",
        "# The 3 id of the previous document got updated with the new document\n",
        "vector_db.update_document(document=updating_new_document, document_id = '3')\n",
        "print(\"data After updation\", vector_db.get(include = ['documents']))\n"
      ],
      "metadata": {
        "id": "KKLffMY-vqqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Delete a Document(inputs) from a vector DB\n",
        "vector_db.delete(ids='2')\n",
        "print(\"Deleted chunk\", vector_db.get(include = ['documents']))\n"
      ],
      "metadata": {
        "id": "eqrcHS2Nvy2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''outputs = Data after Adding to vector store {'ids': ['0', '1', '2', '3'],\n",
        "'embeddings': None, 'metadatas': None, 'documents': ['Rag is genrally called as retrieval agumentation and generation',\n",
        "'Rag has two main important components called as retriver and generator', 'Rag is used to build a chat bot based on your\n",
        " own documents', 'Rag uses LLM which takes query and context as input and generates answer.'],\n",
        " 'uris': None, 'data': None, 'included': ['documents']}\n",
        "\n",
        " data After updation {'ids': ['0', '1', '2', '3'], 'embeddings': None, 'metadatas': None,\n",
        " 'documents': ['Rag is genrally called as retrieval agumentation and generation',\n",
        " 'Rag has two main important components called as retriver and generator',\n",
        " 'Rag is used to build a chat bot based on your own documents', 'Vector db is used to store embeddings'],\n",
        " 'uris': None, 'data': None, 'included': ['documents']}\n",
        "\n",
        " Deleted chunk {'ids': ['0', '1', '3'], 'embeddings': None, 'metadatas': None,\n",
        " 'documents': ['Rag is genrally called as retrieval agumentation and generation',\n",
        " 'Rag has two main important components called as retriver and generator', 'Vector db is used to store embeddings'],\n",
        " 'uris': None, 'data': None, 'included': ['documents']}'''"
      ],
      "metadata": {
        "id": "VaAxsPITv23a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}